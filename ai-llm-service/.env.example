# AI LLM Service Configuration - 3B Optimized
# This configuration uses the lighter, faster 3B model instead of the heavier 8B model
# The 3B model provides approximately 2x faster inference while maintaining good accuracy

# Ollama API Configuration
OLLAMA_BASE_URL=http://localhost:11434

# Model Selection - 3B Optimized for CPU/Low-resource devices
OLLAMA_MODEL=llama3.2:3b

# Fast Model - Use lightweight 3B variant
OLLAMA_FAST_MODEL=llama3.2:3b

# Timeout Configuration (seconds)
# Reduced to 60s for fast 3B model (faster inference)
# Increase to 120s if experiencing timeouts with complex prompts
OLLAMA_TIMEOUT=60

# Application Settings
LOG_LEVEL=INFO
DEBUG=false

# Optional: API Keys and Security
# API_KEY=your_api_key_here
# SECRET_KEY=your_secret_key_here
