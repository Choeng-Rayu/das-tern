# ðŸš€ Quick Start - AI Service with 3B Model (Optimized)

## One-Minute Setup

### Step 1: Get the 3B Model
```bash
ollama pull llama3.2:3b
```
This downloads ~3.8GB (first time only)

### Step 2: Start Ollama
```bash
ollama serve
```
Keep this running in a terminal

### Step 3: Start AI Service
```bash
# In another terminal
cd /Users/macbook/CADT/DasTern/ai-llm-service
export OLLAMA_MODEL=llama3.2:3b
python -m uvicorn app.main_ollama:app --host 0.0.0.0 --port 8001 --reload
```

### Step 4: Verify It Works
```bash
curl http://localhost:8001/health
```

Should return:
```json
{"status": "healthy", "service": "ollama-ai-service", "ollama_connected": true}
```

---

## What Changed?

| Item | Before | After |
|------|--------|-------|
| Model | llama3.1:8b | llama3.2:3b âœ¨ |
| Response Time | 40-120s | 10-30s âš¡ |
| Memory Needed | 6GB | 2GB ðŸ“‰ |
| Max Tokens | 2000 | 1000 |
| Timeout | 300s | 60s |

---

## API Endpoints

### Health Check
```bash
GET http://localhost:8001/health
```

### Extract Prescription & Generate Reminders
```bash
POST http://localhost:8001/api/v1/prescription/enhance-and-generate-reminders
Content-Type: application/json

{
  "ocr_data": {
    "raw_text": "Aspirin 500mg | áž–áŸ’ážšáž¹áž€ | áž›áŸ’áž„áž¶áž…"
  },
  "patient_id": "P12345",
  "base_date": "2026-02-08"
}
```

Response:
```json
{
  "success": true,
  "medications": [
    {
      "name": "Aspirin",
      "dosage": "500mg",
      "times": ["morning", "evening"],
      "times_24h": ["08:00", "18:00"],
      "repeat": "daily"
    }
  ],
  "metadata": {
    "model": "llama3.2:3b",
    "inference_time_ms": 18500
  }
}
```

---

## Environment Variables

Optional (defaults are set):
```bash
export OLLAMA_MODEL=llama3.2:3b          # Model to use
export OLLAMA_TIMEOUT=60                 # Request timeout
export OLLAMA_BASE_URL=http://localhost:11434  # Ollama server
```

Or create `.env` file:
```
OLLAMA_MODEL=llama3.2:3b
OLLAMA_TIMEOUT=60
```

---

## Troubleshooting

**Q: "Connection refused" error?**  
A: Make sure Ollama is running: `ollama serve`

**Q: Getting timeout errors?**  
A: Increase OLLAMA_TIMEOUT: `export OLLAMA_TIMEOUT=120`

**Q: Slow on first request?**  
A: Normal - model is loading into memory. Subsequent requests are faster.

**Q: Want to switch back to 8B?**  
A: `export OLLAMA_MODEL=llama3.1:8b` (needs 6GB+ RAM)

---

## Integration with Backend

The service is ready for backend integration:

**Backend should POST to:**
```
http://ai-llm-service:8001/api/v1/prescription/enhance-and-generate-reminders
```

**Response time:** ~15-25 seconds (vs 40-120s before)

**Data format:** Already optimized and validated

---

## Performance Tips

1. **Warm up the model** on startup with a test request
2. **Use smaller prompts** (already done for 3B)
3. **Monitor memory** - if low, close other apps
4. **Check Ollama status**: `curl http://localhost:11434/api/tags`

---

## Next: Backend Integration

Once running, proceed to configure the backend to use:
- Endpoint: `http://ai-llm-service:8001/api/v1/`
- Timeout: 30-60 seconds
- Expected response time: 15-30 seconds

---

**Status**: âœ… Ready to use  
**Model**: llama3.2:3b (3B Optimized)  
**Performance**: 3-4x faster than 8B  
**Device Compatibility**: Laptop-friendly âœ¨
